{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import horovod.keras as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horovod: initialize Horovod.\n",
    "hvd.init()\n",
    "\n",
    "# Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 173s 15us/step\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "\n",
    "# Horovod: adjust number of epochs based on number of GPUs.\n",
    "epochs = int(math.ceil(12.0 / hvd.size()))\n",
    "\n",
    "# Input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# The data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horovod: adjust learning rate based on number of GPUs.\n",
    "opt = keras.optimizers.Adadelta(1.0 * hvd.size())\n",
    "\n",
    "# Horovod: add Horovod Distributed Optimizer.\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 44s 739us/step - loss: 0.2601 - acc: 0.9194 - val_loss: 0.0558 - val_acc: 0.9816\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 43s 716us/step - loss: 0.0907 - acc: 0.9733 - val_loss: 0.0410 - val_acc: 0.9858\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 43s 712us/step - loss: 0.0676 - acc: 0.9799 - val_loss: 0.0338 - val_acc: 0.9887\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 43s 710us/step - loss: 0.0576 - acc: 0.9829 - val_loss: 0.0333 - val_acc: 0.9888\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 43s 712us/step - loss: 0.0476 - acc: 0.9861 - val_loss: 0.0321 - val_acc: 0.9899\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 43s 720us/step - loss: 0.0430 - acc: 0.9871 - val_loss: 0.0314 - val_acc: 0.9903\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 44s 725us/step - loss: 0.0386 - acc: 0.9886 - val_loss: 0.0295 - val_acc: 0.9903\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 44s 729us/step - loss: 0.0348 - acc: 0.9894 - val_loss: 0.0269 - val_acc: 0.9912\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 44s 733us/step - loss: 0.0329 - acc: 0.9902 - val_loss: 0.0292 - val_acc: 0.9905\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 44s 738us/step - loss: 0.0305 - acc: 0.9907 - val_loss: 0.0316 - val_acc: 0.9899\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 44s 735us/step - loss: 0.0297 - acc: 0.9908 - val_loss: 0.0270 - val_acc: 0.9918\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 44s 735us/step - loss: 0.0287 - acc: 0.9911 - val_loss: 0.0281 - val_acc: 0.9916\n",
      "Test loss: 0.028076594341508464\n",
      "Test accuracy: 0.9916\n"
     ]
    }
   ],
   "source": [
    "# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n",
    "if hvd.rank() == 0:\n",
    "    callbacks.append(keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          callbacks=callbacks,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
